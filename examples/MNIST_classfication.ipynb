{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d067846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neura\n",
    "import neura.nn as nn\n",
    "import neura.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c942d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MNIST dataset...\n",
      "Dataset fetched.\n",
      "Fetching MNIST dataset...\n",
      "Dataset fetched.\n"
     ]
    }
   ],
   "source": [
    "class MNISTDataset(neura.data.Dataset):\n",
    "    def __init__(self, train=True, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the MNIST dataset.\n",
    "\n",
    "        Args:\n",
    "            train (bool): If True, loads the training data, otherwise loads test data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "\n",
    "        # Fetch the data. It's a large download the first time.\n",
    "        print(\"Fetching MNIST dataset...\")\n",
    "        # fetch_openml is a reliable way to get the original MNIST data\n",
    "        mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "        print(\"Dataset fetched.\")\n",
    "\n",
    "        # The data is in a dictionary-like object\n",
    "        # Images are 784-dimensional vectors (28*28)\n",
    "        # Labels are strings '0', '1', ...\n",
    "        images = mnist.data\n",
    "        labels = mnist.target\n",
    "\n",
    "        # Preprocessing Steps\n",
    "        # 1. Normalize pixel values from [0, 255] to [0, 1.0]\n",
    "        images = images / 255.0\n",
    "        # 2. Convert labels from strings to integers\n",
    "        labels = labels.astype(int)\n",
    "        \n",
    "        # 3. Cast data to a more memory-efficient type if desired\n",
    "        images = images.astype(np.float32)\n",
    "\n",
    "        # Split into training and testing sets (standard MNIST split is 60k/10k)\n",
    "        if train:\n",
    "            self.images = images[:60000]\n",
    "            self.labels = labels[:60000]\n",
    "        else:\n",
    "            self.images = images[60000:]\n",
    "            self.labels = labels[60000:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple of (image, label) for a given index.\n",
    "        The image is now reshaped to (1, 28, 28).\n",
    "        \"\"\"\n",
    "        # Get the flattened image vector (784,)\n",
    "        image_flat = self.images[index]\n",
    "        \n",
    "        # Reshape the flat vector into a 3D tensor: (Channels, Height, Width)\n",
    "        image_reshaped = image_flat.reshape(1, 28, 28)\n",
    "    \n",
    "        label = self.labels[index]\n",
    "    \n",
    "        if self.transform:\n",
    "            # Note: transforms would now operate on a 3D tensor\n",
    "            image_reshaped = self.transform(image_reshaped)\n",
    "    \n",
    "        return image_reshaped, label\n",
    "    \n",
    "\n",
    "trainset = MNISTDataset()\n",
    "testset = MNISTDataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db5c3ca-bfa0-48cf-9d44-80b804fdfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = neura.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = neura.data.DataLoader(testset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d40fc5-30d4-42fd-96fd-155f6289a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Feature Extractor ---\n",
    "        # Block 1: Input (B, 1, 28, 28) -> Output (B, 6, 13, 13)\n",
    "        self.conv_block1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(m=6)\n",
    "        \n",
    "        # Block 2: Input (B, 6, 13, 13) -> Output (B, 16, 5, 5)\n",
    "        self.conv_block2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=2, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(m=16)\n",
    "\n",
    "        # --- Classifier (using only Conv2d layers) ---\n",
    "        # Input (B, 16, 5, 5)\n",
    "        \n",
    "        # Layer 1 (replaces fc1): Output (B, 120, 1, 1)\n",
    "        # Using a 5x5 kernel to collapse the spatial dimensions\n",
    "        self.classifier1 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5)\n",
    "        self.bn_class1 = nn.BatchNorm2d(m=120)\n",
    "        \n",
    "        # Layer 2 (replaces fc2): Output (B, 84, 1, 1)\n",
    "        # Input is now 1x1, so we use a 1x1 kernel\n",
    "        self.classifier2 = nn.Conv2d(in_channels=120, out_channels=84, kernel_size=1)\n",
    "        self.bn_class2 = nn.BatchNorm2d(m=84)\n",
    "        \n",
    "        # Layer 3 (replaces fc3 - output layer): Output (B, 10, 1, 1)\n",
    "        self.output_layer = nn.Conv2d(in_channels=84, out_channels=10, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: neura.Tensor) -> neura.Tensor:\n",
    "        x = x.view(x.shape[0], 1, 28, 28)\n",
    "    \n",
    "        # Now the rest of the forward pass will work correctly\n",
    "        # because x is a 4D tensor.\n",
    "        \n",
    "        # Feature Extractor\n",
    "        x = nn.ReLU(self.bn1(self.conv_block1(x)))()\n",
    "        x = nn.ReLU(self.bn2(self.conv_block2(x)))()\n",
    "        \n",
    "        # Classifier\n",
    "        x = nn.ReLU(self.bn_class1(self.classifier1(x)))()\n",
    "        x = nn.ReLU(self.bn_class2(self.classifier2(x)))()\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        # Reshape the output from (B, C, 1, 1) to (B, C) for the loss function\n",
    "        logits = x.view(x.shape[0], -1)\n",
    "        \n",
    "        return logits\n",
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45dc7d0-3ce7-4103-9936-ad14f57dd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitLoss(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f8fe9d-d4a9-4275-982a-b114df1f658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function for one-hot encoding\n",
    "def to_one_hot(labels: neura.Tensor, num_classes=10):\n",
    "    \"\"\"Converts a batch of labels to one-hot encoding.\"\"\"\n",
    "    \n",
    "    # Get the flattened numpy array from the tensor\n",
    "    labels_flat = labels.data.flatten()\n",
    "    \n",
    "    # ===================================================================\n",
    "    # THE FIX IS HERE:\n",
    "    # Ensure the labels are integers before using them for indexing.\n",
    "    # ===================================================================\n",
    "    labels_int = labels_flat.astype(int)\n",
    "    \n",
    "    # Now, use the integer labels for indexing\n",
    "    one_hot = np.zeros((labels_int.shape[0], num_classes))\n",
    "    one_hot[np.arange(labels_int.shape[0]), labels_int] = 1\n",
    "    \n",
    "    # Return a new tensor. Note: The one-hot labels should be floats\n",
    "    # to be compatible with the model's float outputs for the loss function.\n",
    "    return neura.Tensor(one_hot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4cfc4-1ecd-4e4a-82bd-bc37693cd34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n",
      "Epoch 1/5 | Batch [1875/1875] | Loss: 35.3308 | Acc: 12.68%  \n",
      "End of Epoch 1 Summary | Average Loss: 35.3308 | Accuracy: 12.68%\n",
      "Epoch 2/5 | Batch [1875/1875] | Loss: 33.9181 | Acc: 17.30%  \n",
      "End of Epoch 2 Summary | Average Loss: 33.9181 | Accuracy: 17.30%\n",
      "Epoch 3/5 | Batch [1019/1875] | Loss: 33.7817 | Acc: 18.38%  "
     ]
    }
   ],
   "source": [
    "from neura import Tensor\n",
    "\n",
    "# --- BEFORE THE LOOP ---\n",
    "# Make sure to instantiate the criterion with reduction='sum'\n",
    "criterion = nn.BCEWithLogitLoss(reduction=\"sum\") \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        # a. Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # b. Forward pass\n",
    "        outputs = model(inputs) \n",
    "\n",
    "        # c. Prepare labels for BCE Loss\n",
    "        one_hot_labels = to_one_hot(labels, num_classes=10)\n",
    "\n",
    "        # d. Calculate the loss (criterion now returns the sum for the batch)\n",
    "        loss = criterion(outputs, one_hot_labels)\n",
    "\n",
    "        # e. Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # f. Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- Logging and Metrics ---\n",
    "        # Add the sum of loss for the batch to the running total\n",
    "        running_loss += loss.data.item()\n",
    "        \n",
    "        # --- Calculate Accuracy ---\n",
    "        predicted_probs = 1 / (1 + np.exp(-outputs.data))\n",
    "        predicted_labels = np.argmax(predicted_probs, axis=1)\n",
    "        \n",
    "        # FIX 1: Flatten the true labels to prevent broadcasting errors\n",
    "        true_labels = labels.data.flatten()\n",
    "        \n",
    "        correct_predictions += (predicted_labels == true_labels).sum()\n",
    "        total_samples += len(true_labels)\n",
    "        \n",
    "        # --- Real-time Progress Printing ---\n",
    "        # FIX 2: Calculate average loss and accuracy based on total samples seen\n",
    "        avg_loss = running_loss / total_samples\n",
    "        avg_acc = correct_predictions / total_samples\n",
    "        \n",
    "        progress_string = (\n",
    "            f\"Epoch {epoch + 1}/{EPOCHS} | \"\n",
    "            f\"Batch [{batch_idx + 1}/{len(trainloader)}] | \"\n",
    "            f\"Loss: {avg_loss:.4f} | \"\n",
    "            f\"Acc: {avg_acc:.2%}\"\n",
    "        )\n",
    "        print(progress_string + \"  \", end='\\r')\n",
    "\n",
    "    print() # Newline after the epoch\n",
    "\n",
    "    # Final epoch summary is now consistent\n",
    "    final_epoch_loss = running_loss / total_samples\n",
    "    final_epoch_acc = correct_predictions / total_samples\n",
    "    print(f\"End of Epoch {epoch + 1} Summary | Average Loss: {final_epoch_loss:.4f} | Accuracy: {final_epoch_acc:.2%}\")\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f852ab0-0529-45ea-993f-9d22e48965d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c89e0-76eb-4f10-a5e9-768a71b965c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
